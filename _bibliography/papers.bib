---
---
@article{Magnusson2023PalomaAB,
  title={Paloma: A Benchmark for Evaluating Language Model Fit},
  author={Ian Magnusson and Akshita Bhagia and Valentin Hofmann and Luca Soldaini and A. Jha and Oyvind Tafjord and Dustin Schwenk and Evan Pete Walsh and Yanai Elazar and Kyle Lo and Dirk Groeneveld and Iz Beltagy and Hanna Hajishirzi and Noah A. Smith and Kyle Richardson and Jesse Dodge},
  year={2023},
  pdf={https://arxiv.org/abs/2312.10523},
  code={https://github.com/allenai/ai2-olmo-eval/blob/main/paloma/README.md},
  data={https://huggingface.co/datasets/allenai/paloma},
  journal={arXiv preprint},
}

@article{Groeneveld2023CatwalkAU,
  title={Catwalk: A Unified Language Model Evaluation Framework for Many Datasets},
  author={Dirk Groeneveld and Anas Awadalla and Iz Beltagy and Akshita Bhagia and Ian Magnusson and Hao Peng and Oyvind Tafjord and Pete Walsh and Kyle Richardson and Jesse Dodge},
  year={2023},
  pdf={https://arxiv.org/abs/2312.10253},
  code={https://github.com/allenai/catwalk},
  journal={arXiv preprint},
}

@article{Richardson2023GEMabstract,
  title={Robust Tooling and New Resources for Large Language Model Evaluation via Catwalk (extended abstract)},
  author={Kyle Richardson and Ian Magnusson and Oyvind Tafjord and Akshita Bhagia and Iz Beltagy and Arman Cohan and Pradeep Dasigi and Jesse Dodge and Dirk Groeneveld and Yuling Gu and Ananya Harsh Jha and Tushar Khot and Nishant Subramani},
  year={2023},
  journal={<a href="https://gem-benchmark.com/workshop">GEM Workshop</a>, EMNLP},
  code={https://github.com/allenai/ai2-olmo-eval},
}

@article{elazar-wimbd,
  author = {Yanai Elazar and Akshita Bhagia and Ian Magnusson and Abhilasha Ravichander and Dustin Schwenk and Alane Suhr and Pete Walsh and Dirk Groeneveld and Luca Soldaini and Sameer Singh and Hanna Hajishirzi and Noah A. Smith and Jesse Dodge},
  title = {What's In My Big Data?},
  year = {2024},
  pdf = {https://arxiv.org/abs/2310.20707},
  code = {https://github.com/allenai/wimbd},
  website = {https://wimbd.apps.allenai.org/},
  press = {https://www.marktechpost.com/2023/11/05/peeking-inside-pandoras-box-unveiling-the-hidden-complexities-of-language-model-datasets-with-whats-in-my-big-data-wimbd/},
  journal={ICLR (spotlight)},
}

@article{hint,
  author = {Hamish Ivison and Akshita Bhagia and Yizhong Wang and Hannaneh Hajishirzi and Matthew Peters},
  title = {HINT: Hypernetwork Instruction Tuning for Efficient Zero-Shot Generalisation},
  journal = {ACL},
  pdf = {https://arxiv.org/abs/2212.10315},
  year = {2023},
  code={https://github.com/allenai/hyper-task-descriptions},
}

@article{Wu2022,
  title={Continued Pretraining for Better Zero- and Few-Shot Promptability},
  author={Zhaofeng Wu and Robert L. Logan IV and Pete Walsh and Akshita Bhagia and Dirk Groeneveld and Sameer Singh and Iz Beltagy},
  journal={EMNLP},
  year={2022},
  pdf={https://arxiv.org/abs/2210.10258},
  code={https://github.com/allenai/better-promptability},
}

@article{Palaskar2022OnAI,
  title={On Advances in Text Generation from Images Beyond Captioning: A Case Study in Self-Rationalization},
  author={Shruti Palaskar and Akshita Bhagia and Yonatan Bisk and Florian Metze and Alan W. Black and Ana Marasovi{\'c}},
  journal={Findings of EMNLP},
  year={2022},
  pdf={https://arxiv.org/abs/2205.11686},
}

---
---

@article{Muennighoff2024OLMoEOM,
  title={OLMoE: Open Mixture-of-Experts Language Models},
  author={Niklas Muennighoff and Luca Soldaini and Dirk Groeneveld and Kyle Lo and Jacob Daniel Morrison and Sewon Min and Weijia Shi and Pete Walsh and Oyvind Tafjord and Nathan Lambert and Yuling Gu and Shane Arora and Akshita Bhagia and Dustin Schwenk and David Wadden and Alexander Wettig and Binyuan Hui and Tim Dettmers and Douwe Kiela and Ali Farhadi and Noah A. Smith and Pang Wei Koh and Amanpreet Singh and Hanna Hajishirzi},
  year={2024},
  journal={arXiv preprint},
  pdf={https://arxiv.org/abs/2409.02060},
  code={https://github.com/allenai/OLMoE},
  model={https://huggingface.co/allenai/OLMoE-1B-7B-0924},
  press={
    <a href="https://www.marktechpost.com/2024/09/06/olmoe-1b-7b-and-olmoe-1b-7b-instruct-released-a-fully-open-sourced-mixture-of-experts-llm-with-1b-active-and-7b-total-parameters/">MarkTechPost</a>,
    <a href="https://analyticsindiamag.com/ai-news-updates/olmoe-achieves-state-of-the-art-performance-using-fewer-resources-and-moe/">Analytics India Magazine</a>
  },
}

@article{Groeneveld2024OLMoAT,
  title={OLMo: Accelerating the Science of Language Models},
  author={Dirk Groeneveld and Iz Beltagy and Pete Walsh and Akshita Bhagia and Rodney Kinney and Oyvind Tafjord and A. Jha and Hamish Ivison and Ian Magnusson and Yizhong Wang and Shane Arora and David Atkinson and Russell Authur and Khyathi Raghavi Chandu and Arman Cohan and Jennifer Dumas and Yanai Elazar and Yuling Gu and Jack Hessel and Tushar Khot and William Merrill and Jacob Daniel Morrison and Niklas Muennighoff and Aakanksha Naik and Crystal Nam and Matthew E. Peters and Valentina Pyatkin and Abhilasha Ravichander and Dustin Schwenk and Saurabh Shah and Will Smith and Emma Strubell and Nishant Subramani and Mitchell Wortsman and Pradeep Dasigi and Nathan Lambert and Kyle Richardson and Luke Zettlemoyer and Jesse Dodge and Kyle Lo and Luca Soldaini and Noah A. Smith and Hanna Hajishirzi},
  year={2024},
  pdf={https://arxiv.org/abs/2402.00838},
  website={https://allenai.org/olmo},
  code={https://github.com/allenai/OLMo},
  journal = {ACL},
  model = {https://huggingface.co/allenai/OLMo-7B},
  press = {
    <a href="https://www.forbes.com/sites/janakirammsv/2024/02/05/how-olmo-from-the-ai2-redefines-llm-innovation/?sh=50ab7a061472">Forbes</a>, 
    <a href="https://techcrunch.com/2024/02/01/ai2-open-sources-text-generating-ai-models-and-the-data-used-to-train-them/">TechCrunch</a>, 
    <a href="https://www.axios.com/2024/02/01/allen-institute-for-ai-fully-open-source-large-language-model-olmo-7b">Axios</a>, 
    <a href="https://www.geekwire.com/2024/allen-institute-for-ai-promises-new-insights-into-large-language-models-with-olmo-release/">GeekWire</a>, 
    <a href="https://venturebeat.com/ai/truly-open-source-llm-from-ai2-to-drive-critical-shift-in-ai-development/">VentureBeat</a>, 
    <a href="https://www.fastcompany.com/91021305/ai2-new-open-source-llm">FastCompany</a>
  },
  note = {
    <span class="award" href="">üèÜ Best Theme Paper</span><br>
    <span class="award" href="https://www.geekwire.com/2024/geekwire-awards-2024-revealed-winners-bask-in-momentum-of-ai-and-potential-of-region/">üèÜ Geekwire Innovation of the Year</span>
  },
}

@article{Soldaini2024DolmaAO,
  title={Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research},
  author={Luca Soldaini<sup>‚Ä†</sup> and Rodney Kinney<sup>‚Ä†</sup> and Akshita Bhagia<sup>‚Ä†</sup> and Dustin Schwenk<sup>‚Ä†</sup> and David Atkinson and Russell Authur and Ben Bogin and Khyathi Raghavi Chandu and Jennifer Dumas and Yanai Elazar and Valentin Hofmann and A. Jha and Sachin Kumar and Li Lucy and Xinxi Lyu and Nathan Lambert and Ian Magnusson and Jacob Daniel Morrison and Niklas Muennighoff and Aakanksha Naik and Crystal Nam and Matthew E. Peters and Abhilasha Ravichander and Kyle Richardson and Zejiang Shen and Emma Strubell and Nishant Subramani and Oyvind Tafjord and Pete Walsh and Luke Zettlemoyer and Noah A. Smith and Hanna Hajishirzi and Iz Beltagy and Dirk Groeneveld and Jesse Dodge and Kyle Lo<sup>‚Ä†</sup>},
  year={2024},
  journal = {ACL},
  pdf = {https://arxiv.org/abs/2402.00159},
  code={https://github.com/allenai/dolma},
  data={https://huggingface.co/datasets/allenai/dolma},
  press = {
    <a href="https://www.marktechpost.com/2023/08/23/ai2-unveils-dolma-a-3-trillion-token-corpus-pioneering-transparency-in-language-model-research/">MarkTechPost</a>, 
    <a href="https://techcrunch.com/2023/08/18/ai2-drops-biggest-open-dataset-yet-for-training-language-models/">TechCrunch</a>
  },
  note = {
    <span class="award" href="">üèÜ Best Resource Paper</span>
  },
}

@article{elazar-wimbd,
  author = {Yanai Elazar and Akshita Bhagia and Ian Magnusson and Abhilasha Ravichander and Dustin Schwenk and Alane Suhr and Pete Walsh and Dirk Groeneveld and Luca Soldaini and Sameer Singh and Hanna Hajishirzi and Noah A. Smith and Jesse Dodge},
  title = {What's In My Big Data?},
  year = {2024},
  pdf = {https://arxiv.org/abs/2310.20707},
  code = {https://github.com/allenai/wimbd},
  website = {https://wimbd.apps.allenai.org/},
  press = { <a href="https://www.marktechpost.com/2023/11/05/peeking-inside-pandoras-box-unveiling-the-hidden-complexities-of-language-model-datasets-with-whats-in-my-big-data-wimbd/">MarkTechPost</a>},
  journal = {ICLR},
  note = {<span class="award">Spotlight</span>},
}

@article{Magnusson2023PalomaAB,
  title={Paloma: A Benchmark for Evaluating Language Model Fit},
  author={Ian Magnusson and Akshita Bhagia and Valentin Hofmann and Luca Soldaini and A. Jha and Oyvind Tafjord and Dustin Schwenk and Evan Pete Walsh and Yanai Elazar and Kyle Lo and Dirk Groeneveld and Iz Beltagy and Hanna Hajishirzi and Noah A. Smith and Kyle Richardson and Jesse Dodge},
  year={2023},
  pdf={https://arxiv.org/abs/2312.10523},
  code={https://github.com/allenai/OLMo-Eval/blob/main/paloma/README.md},
  data={https://huggingface.co/datasets/allenai/paloma},
  journal={In submission},
}

@article{Groeneveld2023CatwalkAU,
  title={Catwalk: A Unified Language Model Evaluation Framework for Many Datasets},
  author={Dirk Groeneveld and Anas Awadalla and Iz Beltagy and Akshita Bhagia and Ian Magnusson and Hao Peng and Oyvind Tafjord and Pete Walsh and Kyle Richardson and Jesse Dodge},
  year={2023},
  pdf={https://arxiv.org/abs/2312.10253},
  code={https://github.com/allenai/catwalk},
  journal={arXiv preprint},
}

@article{Richardson2023GEMabstract,
  title={Robust Tooling and New Resources for Large Language Model Evaluation via Catwalk (extended abstract)},
  author={Kyle Richardson and Ian Magnusson and Oyvind Tafjord and Akshita Bhagia and Iz Beltagy and Arman Cohan and Pradeep Dasigi and Jesse Dodge and Dirk Groeneveld and Yuling Gu and Ananya Harsh Jha and Tushar Khot and Nishant Subramani},
  year={2023},
  journal={<a href="https://gem-benchmark.com/workshop">GEM Workshop</a>, EMNLP},
  code={https://github.com/allenai/OLMo-Eval},
}

@article{hint,
  author = {Hamish Ivison and Akshita Bhagia and Yizhong Wang and Hannaneh Hajishirzi and Matthew Peters},
  title = {HINT: Hypernetwork Instruction Tuning for Efficient Zero-Shot Generalisation},
  journal = {ACL},
  pdf = {https://arxiv.org/abs/2212.10315},
  year = {2023},
  code={https://github.com/allenai/hyper-task-descriptions},
}

@article{Wu2022,
  title={Continued Pretraining for Better Zero- and Few-Shot Promptability},
  author={Zhaofeng Wu and Robert L. Logan IV and Pete Walsh and Akshita Bhagia and Dirk Groeneveld and Sameer Singh and Iz Beltagy},
  journal={EMNLP},
  year={2022},
  pdf={https://arxiv.org/abs/2210.10258},
  code={https://github.com/allenai/better-promptability},
}

@article{Palaskar2022OnAI,
  title={On Advances in Text Generation from Images Beyond Captioning: A Case Study in Self-Rationalization},
  author={Shruti Palaskar and Akshita Bhagia and Yonatan Bisk and Florian Metze and Alan W. Black and Ana Marasovi{\'c}},
  journal={Findings of EMNLP},
  year={2022},
  pdf={https://arxiv.org/abs/2205.11686},
}
